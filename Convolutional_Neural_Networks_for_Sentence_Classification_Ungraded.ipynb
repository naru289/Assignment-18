{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-18/blob/main/Convolutional_Neural_Networks_for_Sentence_Classification_Ungraded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "## Convolutional Neural Networks for Sentence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        " \n",
        "*  represent words in the sentences using pretrained word embeddings\n",
        "*  generate vector representation of words in the sentence using Glove\n",
        "*  build a convolutional Neural Network for Sentence Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtOp3EVPL-0o"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "#### Description\n",
        "\n",
        "\n",
        "The SICK(Sentences Involving Compositional Knowledge) dataset contains 9840 English sentence pairs:\n",
        "\n",
        "- two sentences, SentenceA and SentenceB\n",
        "- their relation to each other from SentenceA to SentenceB and from SentenceB to SentenceA (entailment, contradiction, neutral)\n",
        "\n",
        "For more details about the dataset refer to the following [link](https://zenodo.org/record/2787612#.YbA5pCbhU8o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C-eklopT0q0"
      },
      "source": [
        "### Problem Statement\n",
        "\n",
        "The aim of this assignment is to study the use of a Siamese neural network for solving the textual\n",
        "entailment problem. We will use the CNN based\n",
        "architecture for sentence representation.\n",
        "\n",
        "Every instance of this dataset consists of two sentences (A and B) and the information whether the sentence (A/B)\n",
        "contradicts/neutral/entails the sentence (B/A). We treat this as a seven-category classification problem\n",
        "and design a Siamese neural network for solving this classification problem. Note that the input to\n",
        "the Siamese network will be two sentences and the output could be a 7-dimensional vector for the\n",
        "7 classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download the Dataset\n",
        "!wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/sententence_data.csv"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ukzpTgs1o2gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVM_u_Czx_jZ"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjtacz2UCmuX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, concatenate, Dense, Bidirectional, Dropout, Flatten, Conv1D, MaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyie_w3yEq_"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2fGPg1t4_ZP"
      },
      "source": [
        "def load_data(dataset):\n",
        "  data = pd.read_csv(dataset)\n",
        "  sentence_data = data.drop('pair_ID', axis=1)\n",
        "  return sentence_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSFQA3ic5Y4o"
      },
      "source": [
        "# Load the dataset\n",
        "sentence_data = load_data('sententence_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgxfwLT55b8-"
      },
      "source": [
        "# Print the first 5 rows from the data\n",
        "sentence_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJTV_7H2CfjZ"
      },
      "source": [
        "# Combining the label columns (entailment_AB, entailment_BA) to create a final labels column\n",
        "sentence_data['labels'] = sentence_data['entailment_AB'].astype(str) + 'and' + sentence_data['entailment_BA'].astype(str)\n",
        "\n",
        "# No of unique labels after combining the entailment_AB and entailment_BA\n",
        "print(len(sentence_data['labels'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJkecxGYZPM"
      },
      "source": [
        "# check for the unique label combinations \n",
        "print(np.unique(sentence_data['labels']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BQlbkoZDfGO"
      },
      "source": [
        "# checking for the output counts \n",
        "sentence_data['labels'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RbrsSj2yJAt"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0HUwo3qRhJo"
      },
      "source": [
        "# Converting the labels from categorical to numerical\n",
        "le = LabelEncoder()\n",
        "sentence_data['labels'] = le.fit_transform(sentence_data['labels'])\n",
        "sentence_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf_6eWLXyUus"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mOVR-b0_7Rp"
      },
      "source": [
        "def cleaning_dataset(df):\n",
        "    \n",
        "    # Pre-Processing\n",
        "    # converat all sentences to string format\n",
        "    df['sentence_A'] = df['sentence_A'].astype(str)\n",
        "    df['sentence_B'] = df['sentence_B'].astype(str)\n",
        "    \n",
        "    # convert all sentences to lower case\n",
        "    df['sentence_A'] = df['sentence_A'].apply(lambda sentence_A: sentence_A.lower())\n",
        "    df['sentence_B'] = df['sentence_B'].apply(lambda sentence_B: sentence_B.lower())\n",
        "       \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAFICGCh_-SR"
      },
      "source": [
        "sentence_data = cleaning_dataset(sentence_data)\n",
        "sentence_data = sentence_data.drop(['entailment_AB','entailment_BA'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JJ3n_8FAkU0"
      },
      "source": [
        "sentence_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLl2mdzBgEO"
      },
      "source": [
        "# Combining both the sentences as a corpus\n",
        "sentence_data['Sentences'] = sentence_data[['sentence_A', 'sentence_B']].apply(lambda x: str(x[0])+\" \"+str(x[1]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BbSPU936D0R"
      },
      "source": [
        "### Tokenize and Pad sequences\n",
        "\n",
        "A Neural Network only accepts numeric data, so we need to encode the reviews. Here use keras.Tokenizer() to encode the reviews into integers, where each unique word is automatically indexed (using `fit_on_texts` method) calculates the frequency of each word in our corpus/messages. \n",
        "\n",
        "`texts_to_sequences` method finally converts our array of sequences of strings to list of sequences of integers (most frequent word is assigned 1 and so on).\n",
        "\n",
        "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using `keras.preprocessing.sequence.pad_sequences.`\n",
        "\n",
        "`post`, pad or truncate the words in the back of a sentence\n",
        "`pre`, pad or truncate the words in front of a sentence\n",
        "\n",
        "Each word is assigned an integer and that integer is placed in a list. \n",
        "\n",
        "\n",
        "For example if we have a sentence “How text to sequence and padding works”. Each word is assigned a number. We suppose how = 1, text = 2, to = 3, sequence = 4, and = 5, padding = 6, works = 7. After texts_to_sequences is called our sentence will look like [1, 2, 3, 4, 5, 6, 7 ]. Now for suppose our MAX_SEQUENCE_LENGTH = 10. After padding our sentence will look like `pre` = [0, 0, 0, 1, 2, 3, 4, 5, 6, 7 ], `post` = [1, 2, 3, 4, 5, 6, 7, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzXMFeMbCtjp"
      },
      "source": [
        "# Tokenizer class from the keras.preprocessing.text module creates a word-to-index integer dictionary\n",
        "# Vectorize the text samples\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentence_data['Sentences'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-KVB44PDGkA"
      },
      "source": [
        "sen_seq_A = tokenizer.texts_to_sequences(sentence_data['sentence_A'])\n",
        "sen_seq_B = tokenizer.texts_to_sequences(sentence_data['sentence_B'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UymMrZNJDR2M"
      },
      "source": [
        "max_len = 50\n",
        "\n",
        "sen_seq_A = pad_sequences(sen_seq_A, maxlen=max_len, padding='post')\n",
        "sen_seq_B = pad_sequences(sen_seq_B, maxlen=max_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkg7rw3VEWsJ"
      },
      "source": [
        "print(sen_seq_A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpXRJXBRDgCW"
      },
      "source": [
        "print(sen_seq_A.shape, sen_seq_B.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMZ9Hb1l-gd"
      },
      "source": [
        "### Load the GloVe word embeddings\n",
        "\n",
        "**What is GloVe?**\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. These are essential for solving most Natural language processing problems.The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n",
        "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "Now, let us load the 50-dimensional GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNHXa8G4DMcm"
      },
      "source": [
        "embeddings_index = {}\n",
        "# Loading the 300-dimensional vector of the model\n",
        "f = open('/content/glove.6B.50d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUbiAQ1D5HI"
      },
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoOwxlsFEFdH"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQmtuo7MEIm4"
      },
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4P8bwYey5MD"
      },
      "source": [
        "### Splitting the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv6ilxXpqgSB"
      },
      "source": [
        "X = np.stack((sen_seq_A, sen_seq_B), axis=1)\n",
        "Y = sentence_data['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4doSO5lrTP5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.25, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW8SvDBF-oqt"
      },
      "source": [
        "# Check for the shape of train and test sets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4FP8JBWs5kk"
      },
      "source": [
        "# Storing the sentence_A and sentence_B data seperately for training\n",
        "train_s1 = X_train[:,0]\n",
        "train_s2 = X_train[:,1]\n",
        "test_s1 = X_test[:,0]\n",
        "test_s2 = X_test[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzlEd11evA_k"
      },
      "source": [
        "# Converting labels to array\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txmoVPF3S119"
      },
      "source": [
        "# one-hot encode the labels\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Juw0huFTD2A"
      },
      "source": [
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4wid6RBzVEs"
      },
      "source": [
        "### Define the Convolutional Neural Network (CNN) model\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/CNN_Sentence.png\" width=800px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "Deep learning models have achieved remarkable results in  computer  vision and speech recognition in recent years.  Within natural language processing,  much of the work with deep learning methods has involved learning word vector representations through neural language models and performing composition over the learned word vectors for classification .Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabularysize) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions.\n",
        "\n",
        "Convolutional neural networks (CNN) utilize layers  with  convolving filters that are  applied to local  features.   Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and  have  achieved  excellent results in semantic parsing, search query retrieval, sentence  modeling, and other traditional NLP tasks.\n",
        "\n",
        "In the present work, we train a simple CNN with one layer of convolution on top of word vectors obtained from  an  unsupervised neural language model. These vectors were trained on 100 billion words of Google News, and are publicly available. We initially keep the word vectors static and learn only the other parameters of the model.\n",
        "\n",
        "From the above figure we can observe that the inputs are words. Each word is represented by a vector of size (50, 100, 200, 300).\n",
        "Apply different filters on the word vectors to create convolutional feature map. Choose the maximum value of the result from each filter vector for pooled representation and then\n",
        "apply softmax to perform classification.\n",
        "\n",
        "\n",
        "We have seen the process by which one feature is extracted  from one filter. The  model uses multiple filters (with varying window sizes) to obtain multiple features. These features form the penultimate layer and are passed to a fully connected and then softmax is used to perform classification.\n",
        "\n",
        "#### 1-D Convolutions over text\n",
        "\n",
        "The application of convolutional neural networks is the same as in image data. The only difference is that 1D convolutions are applied instead of 2D convolutions. In images, the kernel slides in 2D but in sequence data like text data the kernel slides in one dimension. \n",
        "\n",
        "Let’s now take a look at how this CNN can be built. \n",
        "\n",
        "The convolution network will be made of of the following: \n",
        "\n",
        "* An embedding layer that turns the data into dense vectors of fixed size. \n",
        "\n",
        "* A `Conv1D` with no of filter units and the `relu` activation function.\n",
        "\n",
        "* A `MaxPooling1D` layer that downsamples the input by taking the maximum value. The pooling operation is used to combine the vectors resulting from different convolution windows into a single $l$-dimensional vector. This is done again by taking the max or the average value observed in resulting vector from the convolutions. Ideally this vector will capture the most relevant features of the sentence/document.\n",
        "\n",
        "* A `Dense` layer with hidden units for the fully connected layer.\n",
        "    \n",
        "* An output layer with the softmax activation function.\n",
        "\n",
        "\n",
        "\n",
        "**Note:** Refer to the following [link](https://cezannec.github.io/CNN_Text_Classification/) for more details of CNN in Text classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zx8Y4hlSngF"
      },
      "source": [
        "# Define a function which takes the input sequence and generate the vectors using the pretrained word embeddings\n",
        "# The pass it through the convolutional layers and then the max pooling layer\n",
        "def cnn_model(input_shape, vocab_size, embeddings, embedding_dim):\n",
        "  # Input as max sequence length\n",
        "  model_input = Input(shape=(input_shape,))\n",
        "\n",
        "  # Embedding layer\n",
        "  layer = Embedding(vocab_size, \n",
        "                 embedding_dim, \n",
        "                 weights=[embedding_matrix], \n",
        "                 input_length=max_len, \n",
        "                 trainable=False)(model_input)\n",
        "                 \n",
        "  # Convolutional layer              \n",
        "  layer = Conv1D(filters=16, kernel_size=3, activation='relu')(layer)\n",
        "  # MaxPool layer\n",
        "  layer = MaxPooling1D(pool_size=2)(layer)\n",
        "  output = Flatten()(layer)\n",
        "\n",
        "  model = Model(inputs=model_input, outputs=output)\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZgmRlisQqsQ"
      },
      "source": [
        "#### CNN for Sentence Classification\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/CNN_sentence_classification.png\" width=900px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "Here, we tokenize the sentences (sentence_A and sentence_B) into words and then map the text to sequences and for the input sequences we build the word embedding with the pretrained(word2vec, Glove, FastText) models.\n",
        "\n",
        "We pass the two sentence(A and B) to the CNN model which shares the same weight parameters, which provides the enocoded representation of the sentences. Then, we concatenate the output sentences(A and B) vectors pass it to the fully connected layers of neural networks to train the model and perform the classification at the final layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_uLufiSUbuE"
      },
      "source": [
        "# Call the CNN model with max sequence length, embeddings and word embedding dimensions\n",
        "cnn_model = cnn_model(max_len, vocab_size, embedding_matrix, embedding_dim)\n",
        "\n",
        "# sentence_A and sentence_B inputs as a max sequence length\n",
        "input_s1 = Input(shape=(max_len,),dtype='int32')\n",
        "input_s2 = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "# pass the input sentences to the CNN model which share the same weight parameters\n",
        "left_out = cnn_model(input_s1)\n",
        "right_out = cnn_model(input_s2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snqfQtB8eMO8"
      },
      "source": [
        "# Concatenate the sentence_A and sentence_B output vectors from the CNN model\n",
        "merge = concatenate([left_out, right_out], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Ce-9cpUpVw"
      },
      "source": [
        "# Fully connected layers\n",
        "sumx = Dense(256, activation='relu')(merge)\n",
        "sumx = Dense(128, activation='relu')(sumx)\n",
        "sumx = Dense(64, activation='relu')(sumx)\n",
        "sumx = Dropout(0.5)(sumx)\n",
        "\n",
        "# Output layer for 7 class classification\n",
        "pred = Dense(7, activation='softmax')(sumx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFuwU8tl1M57"
      },
      "source": [
        "# Creating the functional model with input sentences and outputs the 7 classes\n",
        "cnn_model = Model(inputs=[input_s1, input_s2], outputs=pred)\n",
        "cnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8wo9lrl1PKC"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t90gkKUfULWC"
      },
      "source": [
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training the CNN model\n",
        "history = cnn_model.fit([train_s1,train_s2],\n",
        "                    y_train, \n",
        "                    epochs=10,\n",
        "                    batch_size=16,\n",
        "                   validation_data=([test_s1, test_s2], y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Y3RDTfYUYX"
      },
      "source": [
        "# Get the predictions on the test set\n",
        "y_pred = cnn_model.predict([test_s1, test_s2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brpATi0fYuzu"
      },
      "source": [
        "yout = np.argmax(y_pred, axis=1)\n",
        "yout.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmzOFgfdYKuo"
      },
      "source": [
        "y_test = np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymUpEMhZBz_"
      },
      "source": [
        "yout.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-cCxjEqeFTP"
      },
      "source": [
        "print(y_test.shape, yout.shape)\n",
        "print(y_test[:5], yout[:5])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}